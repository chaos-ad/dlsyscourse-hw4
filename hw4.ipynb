{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to set up the assignment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "# !mkdir -p 10714\n",
    "# %cd /content/drive/MyDrive/10714\n",
    "# !git clone https://github.com/dlsys10714/hw4.git\n",
    "# %cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/pybind11/include (found version \"2.10.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Sun Nov 20 18:03:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   35C    P8    15W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  7.5\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build\n",
      "make[1]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[2]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[3]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[3]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[3]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-39-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[3]: Entering directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-39-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n",
      "make[1]: Leaving directory `/home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Fill in the following classes in `python/needle/ops.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not need to change your previous solution, however `TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ec2-user/SageMaker/code/dlsyscourse-homework/hw4\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  0%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  1%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  2%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  4%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  6%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  7%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  8%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 10%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 13%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 14%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 15%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 17%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 18%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 19%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 20%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 21%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 23%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 24%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 25%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 26%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 28%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 29%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 30%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 31%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 32%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 34%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 35%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 36%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 37%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 39%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 40%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 41%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 42%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 43%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                   [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 45%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 46%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 47%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 48%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 49%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 51%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 52%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 53%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 54%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 56%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 57%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 58%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 59%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 60%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 62%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 63%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 64%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 65%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 67%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 68%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 69%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 71%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 74%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 75%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 76%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 80%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 81%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 82%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 84%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 85%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 86%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 87%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 89%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 90%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 91%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 92%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 93%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 95%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 96%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 97%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 98%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 99%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m           [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-16-16-16] ___________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 16, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51...959696 -1.3108143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]])\n",
      "B          = needle.Tensor([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.66...65777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]])\n",
      "_A         = array([[-0.15554655, -1.2261544 , -1.3876351 ,  0.3273014 ,  0.45741168,\n",
      "         0.8751152 ,  0.31749257, -0.4368339 ...310244 ,\n",
      "        -0.60321134, -0.50895303,  0.3437534 ,  0.05532323, -0.36882105,\n",
      "         0.01935508]], dtype=float32)\n",
      "_B         = array([[-0.92069906,  0.8202185 , -1.1294544 , -1.6430022 ,  0.385828  ,\n",
      "        -1.3562282 , -0.9987398 , -0.26050347...1863153,\n",
      "        -0.75460845,  0.06351615, -1.9021302 , -1.2813807 , -0.5920339 ,\n",
      "        -0.49041998]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 16\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.66...65777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]])\n",
      "        self       = needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51...959696 -1.3108143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.5...5777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc86757d6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.5...5777  -0.00881964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc86757d6d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86757d940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86757d940>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51824284...143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]], device=cpu())\n",
      "        b          = NDArray([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.6609252 ...1964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc86757d6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168\n",
      "     0.8751152   0.31749257 -0.4368339 ]\n",
      "   [ ... [-0.5683854   0.5310244  -0.60321134 -0.50895303  0.3437534\n",
      "     0.05532323 -0.36882105  0.01935508]]]], device=cpu())\n",
      "        b          = NDArray([[[[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828\n",
      "    -1.3562282  -0.9987398  -0.26050347]\n",
      "   [-0.... [-0.09005287  0.81863153 -0.75460845  0.06351615 -1.9021302\n",
      "    -1.2813807  -0.5920339  -0.49041998]]]], device=cpu())\n",
      "        m          = 16\n",
      "        n          = 16\n",
      "        other      = NDArray([[-0.92069906  0.8202185  -1.1294544  -1.6430022   0.385828   -1.3562282\n",
      "  -0.9987398  -0.26050347  0.6609252 ...1964 -0.09005287  0.81863153 -0.75460845  0.06351615\n",
      "  -1.9021302  -1.2813807  -0.5920339  -0.49041998]], device=cpu())\n",
      "        out        = NDArray([[[[ -2.4361374    7.2991      -4.8236933    2.3496153    7.1879835\n",
      "      4.1394386    1.4709624    4.1424303 ...834155   0.8836147    0.17019045   3.5331821   -4.9130154\n",
      "     -4.9656544    1.3924993   -2.160125  ]]]], device=cpu())\n",
      "        p          = 16\n",
      "        self       = NDArray([[-0.15554655 -1.2261544  -1.3876351   0.3273014   0.45741168  0.8751152\n",
      "   0.31749257 -0.4368339   0.51824284...143  -0.5683854   0.5310244  -0.60321134 -0.50895303\n",
      "   0.3437534   0.05532323 -0.36882105  0.01935508]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc8ed5e74c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[ -2.4361374    7.2991      -4.8236933    2.3496153    7.1879835\n",
      "      4.1394386    1.4709624    4.1424303 ...834155   0.8836147    0.17019045   3.5331821   -4.9130154\n",
      "     -4.9656544    1.3924993   -2.160125  ]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[ -2.4361374    7.2991      -4.8236933    2.3496153    7.1879835\n",
      "      4.1394386    1.4709624    4.1424303 ...834155   0.8836147    0.17019045   3.5331821   -4.9130154\n",
      "     -4.9656544    1.3924993   -2.160125  ]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_matmul[cpu-8-8-8] ____________________________\u001b[0m\n",
      "\n",
      "m = 8, n = 8, p = 8, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1...3101  -0.24976124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]])\n",
      "B          = needle.Tensor([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-...957   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]])\n",
      "_A         = array([[ 0.48591456,  1.177145  , -1.466795  ,  1.6390375 , -0.7933805 ,\n",
      "        -0.4654586 ,  1.0839599 , -2.443315  ...45,  0.5460881 , -0.39847532, -1.4903992 , -0.36902663,\n",
      "         0.529036  , -0.86960596, -1.1056118 ]], dtype=float32)\n",
      "_B         = array([[-1.0530033 , -0.28650165, -0.17814207, -0.11176766, -1.3285084 ,\n",
      "        -0.02897426, -0.6904589 , -1.0077598 ...4 , -0.03515908,  1.3605766 , -0.35897616, -0.72593737,\n",
      "        -1.1327294 , -1.3635881 ,  0.8229569 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 8\n",
      "n          = 8\n",
      "p          = 8\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-...957   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]])\n",
      "        self       = needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1...3101  -0.24976124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-...57   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc86001a7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-...57   1.2171819 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc86001a7c0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86001a670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86001a670>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1.49407...124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]], device=cpu())\n",
      "        b          = NDArray([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-1.9156...9 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc86001a7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805\n",
      "    -0.4654586   1.0839599  -2.443315  ]\n",
      "   [-1...[ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663\n",
      "     0.529036   -0.86960596 -1.1056118 ]]]], device=cpu())\n",
      "        b          = NDArray([[[[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084\n",
      "    -0.02897426 -0.6904589  -1.0077598 ]\n",
      "   [-1...[-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737\n",
      "    -1.1327294  -1.3635881   0.8229569 ]]]], device=cpu())\n",
      "        m          = 8\n",
      "        n          = 8\n",
      "        other      = NDArray([[-1.0530033  -0.28650165 -0.17814207 -0.11176766 -1.3285084  -0.02897426\n",
      "  -0.6904589  -1.0077598 ]\n",
      " [-1.9156...9 ]\n",
      " [-0.7927324  -0.03515908  1.3605766  -0.35897616 -0.72593737 -1.1327294\n",
      "  -1.3635881   0.8229569 ]], device=cpu())\n",
      "        out        = NDArray([[[[-2.1357296  -3.1089735  -4.9851003   1.372391   -0.0854243\n",
      "     4.7113276   6.748822   -0.6505009 ]\n",
      "   [ 2... [ 0.33414415  1.7231225  -4.6190205  -0.25963613  1.2151922\n",
      "     0.59553343  1.6075305  -4.018143  ]]]], device=cpu())\n",
      "        p          = 8\n",
      "        self       = NDArray([[ 0.48591456  1.177145   -1.466795    1.6390375  -0.7933805  -0.4654586\n",
      "   1.0839599  -2.443315  ]\n",
      " [-1.49407...124]\n",
      " [ 0.14081745  0.5460881  -0.39847532 -1.4903992  -0.36902663  0.529036\n",
      "  -0.86960596 -1.1056118 ]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc8675c90d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[-2.1357296  -3.1089735  -4.9851003   1.372391   -0.0854243\n",
      "     4.7113276   6.748822   -0.6505009 ]\n",
      "   [ 2... [ 0.33414415  1.7231225  -4.6190205  -0.25963613  1.2151922\n",
      "     0.59553343  1.6075305  -4.018143  ]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[-2.1357296  -3.1089735  -4.9851003   1.372391   -0.0854243\n",
      "     4.7113276   6.748822   -0.6505009 ]\n",
      "   [ 2... [ 0.33414415  1.7231225  -4.6190205  -0.25963613  1.2151922\n",
      "     0.59553343  1.6075305  -4.018143  ]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-16-16-32] ___________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 32, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1...16476  -0.3986269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]])\n",
      "B          = needle.Tensor([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0...00  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]])\n",
      "_A         = array([[ 0.59100014,  1.8669811 , -1.8348835 ,  0.37579638,  1.1054361 ,\n",
      "         0.40346032, -0.37935227,  0.25318664...8514323,\n",
      "         0.11035689, -0.14837153, -1.6391121 ,  0.83434004,  0.69749874,\n",
      "        -0.38745925]], dtype=float32)\n",
      "_B         = array([[-1.14337194e+00,  2.49391809e-01,  1.08130765e+00,\n",
      "        -1.70937479e+00, -1.10590553e+00,  7.21969530e-02,\n",
      "...,\n",
      "        -4.79118615e-01,  6.65913820e-01, -1.02438211e+00,\n",
      "        -7.88112700e-01,  1.99212804e-01]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 16\n",
      "n          = 16\n",
      "p          = 32\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0...00  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]])\n",
      "        self       = needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1...16476  -0.3986269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0....0  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c719d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0....0  5.66456497e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc845c719d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c71a60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c71a60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1862415...269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]], device=cpu())\n",
      "        b          = NDArray([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0922144...e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c719d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361\n",
      "     0.40346032 -0.37935227  0.25318664]\n",
      "   [-0... [-0.43782985  0.28514323  0.11035689 -0.14837153 -1.6391121\n",
      "     0.83434004  0.69749874 -0.38745925]]]], device=cpu())\n",
      "        b          = NDArray([[[[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "    -1.10590553e+00  7.21969530e-02 -8.092... -1.13789868e+00 -4.79118615e-01\n",
      "     6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]]]], device=cpu())\n",
      "        m          = 16\n",
      "        n          = 16\n",
      "        other      = NDArray([[-1.14337194e+00  2.49391809e-01  1.08130765e+00 -1.70937479e+00\n",
      "  -1.10590553e+00  7.21969530e-02 -8.0922144...e-01 -1.13789868e+00 -4.79118615e-01\n",
      "   6.65913820e-01 -1.02438211e+00 -7.88112700e-01  1.99212804e-01]], device=cpu())\n",
      "        out        = NDArray([[[[-6.16105080e+00  1.43343163e+00 -1.18090940e+00 -4.49024153e+00\n",
      "    -6.53586507e-01  7.85340011e-01  7.496...  3.26508451e+00  4.78280020e+00\n",
      "     2.25645924e+00 -3.50232053e+00 -5.38369417e-01  8.08236504e+00]]]], device=cpu())\n",
      "        p          = 32\n",
      "        self       = NDArray([[ 0.59100014  1.8669811  -1.8348835   0.37579638  1.1054361   0.40346032\n",
      "  -0.37935227  0.25318664  0.1862415...269  -0.43782985  0.28514323  0.11035689 -0.14837153\n",
      "  -1.6391121   0.83434004  0.69749874 -0.38745925]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc8675f7f70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[-6.16105080e+00  1.43343163e+00 -1.18090940e+00 -4.49024153e+00\n",
      "    -6.53586507e-01  7.85340011e-01  7.496...  3.26508451e+00  4.78280020e+00\n",
      "     2.25645924e+00 -3.50232053e+00 -5.38369417e-01  8.08236504e+00]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[-6.16105080e+00  1.43343163e+00 -1.18090940e+00 -4.49024153e+00\n",
      "    -6.53586507e-01  7.85340011e-01  7.496...  3.26508451e+00  4.78280020e+00\n",
      "     2.25645924e+00 -3.50232053e+00 -5.38369417e-01  8.08236504e+00]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-64-64-64] ___________________________\u001b[0m\n",
      "\n",
      "m = 64, n = 64, p = 64, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931....09954476 -0.16885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]])\n",
      "B          = needle.Tensor([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2....03 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]])\n",
      "_A         = array([[-0.28366467,  1.2510917 , -0.01465345, ..., -0.708219  ,\n",
      "        -1.5997269 , -1.0858743 ],\n",
      "       [-1.1571369... ],\n",
      "       [-2.1042256 , -0.13245133, -0.45385894, ..., -2.5528316 ,\n",
      "         0.04279845, -0.29738396]], dtype=float32)\n",
      "_B         = array([[ 1.5264846e-01, -1.4349681e+00,  4.3614903e-03, ...,\n",
      "         1.2838923e+00,  7.0336229e-01,  1.1196541e-01],\n",
      "...4082e+00, -3.1304705e-01,  1.2746793e+00, ...,\n",
      "         6.5945369e-01,  4.2147583e-01,  1.5114847e+00]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 64\n",
      "n          = 64\n",
      "p          = 64\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2....03 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]])\n",
      "        self       = needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931....09954476 -0.16885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.911993...3 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c73f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.911993...3 -1.9393919e-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc845c73f10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c73820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c73820>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931  -2.5...6885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]], device=cpu())\n",
      "        b          = NDArray([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2.045734...-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c73f10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[-2.83664674e-01  1.25109172e+00 -1.46534536e-02 ... -5.76889932e-01\n",
      "     7.95695364e-01  1.25558758e+00]\n",
      " ...12339139e+00 -1.69021308e+00 -1.57776392e+00 ... -2.55283165e+00\n",
      "     4.27984484e-02 -2.97383964e-01]]]], device=cpu())\n",
      "        b          = NDArray([[[[ 1.52648464e-01 -1.43496811e+00  4.36149025e-03 ... -2.42814660e+00\n",
      "     3.88297111e-01  2.11607361e+00]\n",
      " ...07717228e+00  3.06295514e-01 -9.01424706e-01 ...  6.59453690e-01\n",
      "     4.21475828e-01  1.51148474e+00]]]], device=cpu())\n",
      "        m          = 64\n",
      "        n          = 64\n",
      "        other      = NDArray([[ 1.5264846e-01 -1.4349681e+00  4.3614903e-03 ...  1.2838923e+00\n",
      "   7.0336229e-01  1.1196541e-01]\n",
      " [ 2.045734...-02]\n",
      " [-1.1024082e+00 -3.1304705e-01  1.2746793e+00 ...  6.5945369e-01\n",
      "   4.2147583e-01  1.5114847e+00]], device=cpu())\n",
      "        out        = NDArray([[[[-5.55153751e+00 -1.03561258e+01 -5.18424511e+00 ...  8.78402805e+00\n",
      "     1.44363117e+01 -1.21638513e+00]\n",
      " ...05140076e+01 -6.60271311e+00 -9.35861588e+00 ... -9.80177212e+00\n",
      "     1.82429194e+00 -3.82314157e+00]]]], device=cpu())\n",
      "        p          = 64\n",
      "        self       = NDArray([[-0.28366467  1.2510917  -0.01465345 ... -0.708219   -1.5997269\n",
      "  -1.0858743 ]\n",
      " [-1.1571369  -1.9119931  -2.5...6885027\n",
      "  -0.2995055 ]\n",
      " [-2.1042256  -0.13245133 -0.45385894 ... -2.5528316   0.04279845\n",
      "  -0.29738396]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc8675c91f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[-5.55153751e+00 -1.03561258e+01 -5.18424511e+00 ...  8.78402805e+00\n",
      "     1.44363117e+01 -1.21638513e+00]\n",
      " ...05140076e+01 -6.60271311e+00 -9.35861588e+00 ... -9.80177212e+00\n",
      "     1.82429194e+00 -3.82314157e+00]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[-5.55153751e+00 -1.03561258e+01 -5.18424511e+00 ...  8.78402805e+00\n",
      "     1.44363117e+01 -1.21638513e+00]\n",
      " ...05140076e+01 -6.60271311e+00 -9.35861588e+00 ... -9.80177212e+00\n",
      "     1.82429194e+00 -3.82314157e+00]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[cpu-72-72-72] ___________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 72, p = 72, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.038116...0.59203815  0.15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]])\n",
      "B          = needle.Tensor([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.171024...-2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]])\n",
      "_A         = array([[ 0.4375507 , -0.8506252 ,  0.63876194, ..., -0.25451216,\n",
      "        -0.31289756,  0.04666944],\n",
      "       [-0.4258043... ],\n",
      "       [-0.84942585,  0.3735221 , -0.31763205, ..., -1.2876897 ,\n",
      "        -1.1643226 , -0.42393202]], dtype=float32)\n",
      "_B         = array([[-0.34177306,  0.80441475,  1.7493241 , ...,  2.3223765 ,\n",
      "        -0.22549094,  0.87994814],\n",
      "       [ 0.6240802... ],\n",
      "       [-0.16189983,  0.03018835, -1.2238582 , ...,  0.33606574,\n",
      "        -0.6597182 ,  3.3332303 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 72\n",
      "n          = 72\n",
      "p          = 72\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.171024...-2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]])\n",
      "        self       = needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.038116...0.59203815  0.15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.03811...2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845b89a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.03811...2.3511746  -3.2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc845b89a90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b89e80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b89e80>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.0381164   0....15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]], device=cpu())\n",
      "        b          = NDArray([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.17102477  1.....2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845b89a90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[ 4.37550694e-01 -8.50625217e-01  6.38761938e-01 ...  1.41356754e+00\n",
      "    -7.10696101e-01 -7.22515821e-01]\n",
      " ...15897426e-01 -2.50959635e+00 -1.84090078e+00 ... -1.28768969e+00\n",
      "    -1.16432261e+00 -4.23932016e-01]]]], device=cpu())\n",
      "        b          = NDArray([[[[-3.41773063e-01  8.04414749e-01  1.74932408e+00 ... -7.29053140e-01\n",
      "    -4.42871183e-01 -4.86704819e-02]\n",
      " ...75085172e-01 -4.65794653e-01  1.98040557e+00 ...  3.36065739e-01\n",
      "    -6.59718215e-01  3.33323026e+00]]]], device=cpu())\n",
      "        m          = 72\n",
      "        n          = 72\n",
      "        other      = NDArray([[-0.34177306  0.80441475  1.7493241  ...  2.3223765  -0.22549094\n",
      "   0.87994814]\n",
      " [ 0.62408024  0.17102477  1.....2595842\n",
      "   0.2255708 ]\n",
      " [-0.16189983  0.03018835 -1.2238582  ...  0.33606574 -0.6597182\n",
      "   3.3332303 ]], device=cpu())\n",
      "        out        = NDArray([[[[ 3.91188526e+00 -1.03078747e+01  1.37833989e+00 ...  1.55012727e-01\n",
      "     1.84885824e+00 -1.82533226e+01]\n",
      " ...86701069e+01  1.37800407e+01 -6.66155005e+00 ... -1.53865099e+00\n",
      "     5.31516647e+00 -8.30301857e+00]]]], device=cpu())\n",
      "        p          = 72\n",
      "        self       = NDArray([[ 0.4375507  -0.8506252   0.63876194 ... -0.25451216 -0.31289756\n",
      "   0.04666944]\n",
      " [-0.42580435  0.0381164   0....15285783\n",
      "  -1.1825513 ]\n",
      " [-0.84942585  0.3735221  -0.31763205 ... -1.2876897  -1.1643226\n",
      "  -0.42393202]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc8675f7e50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[ 3.91188526e+00 -1.03078747e+01  1.37833989e+00 ...  1.55012727e-01\n",
      "     1.84885824e+00 -1.82533226e+01]\n",
      " ...86701069e+01  1.37800407e+01 -6.66155005e+00 ... -1.53865099e+00\n",
      "     5.31516647e+00 -8.30301857e+00]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[ 3.91188526e+00 -1.03078747e+01  1.37833989e+00 ...  1.55012727e-01\n",
      "     1.84885824e+00 -1.82533226e+01]\n",
      " ...86701069e+01  1.37800407e+01 -6.66155005e+00 ... -1.53865099e+00\n",
      "     5.31516647e+00 -8.30301857e+00]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_matmul[cpu-128-128-128] _________________________\u001b[0m\n",
      "\n",
      "m = 128, n = 128, p = 128, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\n",
      "        _A = np.random.randn(m, n).astype(np.float32)\n",
      "        _B = np.random.randn(n, p).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\n",
      ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924 ...0.20054239 -1.1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]])\n",
      "B          = needle.Tensor([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.3463737...1.8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]])\n",
      "_A         = array([[ 0.22023457, -0.5929843 , -0.647192  , ...,  0.8990233 ,\n",
      "        -0.0346416 ,  0.39623448],\n",
      "       [-0.4756653...9],\n",
      "       [-0.30575415,  1.0274688 ,  2.1192799 , ...,  0.05281937,\n",
      "         0.19285347, -1.8073474 ]], dtype=float32)\n",
      "_B         = array([[ 1.7749221 , -1.5313677 , -0.1591712 , ..., -0.13379207,\n",
      "        -0.3885827 ,  0.37725407],\n",
      "       [-2.2680979...3],\n",
      "       [ 0.45524272, -0.43281022,  1.1288149 , ..., -0.5228368 ,\n",
      "        -0.2814408 , -0.6886872 ]], dtype=float32)\n",
      "device     = cpu()\n",
      "m          = 128\n",
      "n          = 128\n",
      "p          = 128\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:93: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:343: in __matmul__\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\n",
      "        other      = needle.Tensor([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.3463737...1.8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]])\n",
      "        self       = needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924 ...0.20054239 -1.1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924....8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]]))\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c717f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924....8209276  -0.09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]]))\n",
      "        op         = <needle.ops.MatMul object at 0x7fc845c717f0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c719a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c719a0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:314: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = NDArray([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924   -0.7...1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]], device=cpu())\n",
      "        b          = NDArray([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.34637374  0.8...09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]], device=cpu())\n",
      "        self       = <needle.ops.MatMul object at 0x7fc845c717f0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:519: in __matmul__\n",
      "    out.permute((\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m))\n",
      "        a          = NDArray([[[[ 2.20234573e-01 -5.92984319e-01 -6.47192001e-01 ... -1.00535524e+00\n",
      "     5.87355077e-01 -8.91188204e-01]\n",
      " ...14670444e+00 -7.49945164e-01  5.64772412e-02 ...  5.28193749e-02\n",
      "     1.92853466e-01 -1.80734742e+00]]]], device=cpu())\n",
      "        b          = NDArray([[[[ 1.77492213e+00 -1.53136766e+00 -1.59171194e-01 ... -6.53858781e-01\n",
      "     8.16863924e-02 -1.39890361e+00]\n",
      " ...39537114e-01 -6.77922785e-01  1.02387145e-02 ... -5.22836804e-01\n",
      "    -2.81440794e-01 -6.88687205e-01]]]], device=cpu())\n",
      "        m          = 128\n",
      "        n          = 128\n",
      "        other      = NDArray([[ 1.7749221  -1.5313677  -0.1591712  ... -0.13379207 -0.3885827\n",
      "   0.37725407]\n",
      " [-2.2680979  -0.34637374  0.8...09116117\n",
      "   0.49744523]\n",
      " [ 0.45524272 -0.43281022  1.1288149  ... -0.5228368  -0.2814408\n",
      "  -0.6886872 ]], device=cpu())\n",
      "        out        = NDArray([[[[-1.00357752e+01 -5.57393456e+00  9.04624271e+00 ...  6.69985342e+00\n",
      "    -4.97491550e+00 -1.21312981e+01]\n",
      " ...88357639e+00  7.89493847e+00 -1.16166964e+01 ...  3.73075867e+01\n",
      "     4.10082293e+00  1.48650608e+01]]]], device=cpu())\n",
      "        p          = 128\n",
      "        self       = NDArray([[ 0.22023457 -0.5929843  -0.647192   ...  0.8990233  -0.0346416\n",
      "   0.39623448]\n",
      " [-0.47566533  1.197924   -0.7...1254958\n",
      "  -0.12099399]\n",
      " [-0.30575415  1.0274688   2.1192799  ...  0.05281937  0.19285347\n",
      "  -1.8073474 ]], device=cpu())\n",
      "        t          = 8\n",
      "        tile       = <function NDArray.__matmul__.<locals>.tile at 0x7fc845ca51f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[-1.00357752e+01 -5.57393456e+00  9.04624271e+00 ...  6.69985342e+00\n",
      "    -4.97491550e+00 -1.21312981e+01]\n",
      " ...88357639e+00  7.89493847e+00 -1.16166964e+01 ...  3.73075867e+01\n",
      "     4.10082293e+00  1.48650608e+01]]]], device=cpu())\n",
      "new_axes = (0, 2, 1, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1, 3)\n",
      "self       = NDArray([[[[-1.00357752e+01 -5.57393456e+00  9.04624271e+00 ...  6.69985342e+00\n",
      "    -4.97491550e+00 -1.21312981e+01]\n",
      " ...88357639e+00  7.89493847e+00 -1.16166964e+01 ...  3.73075867e+01\n",
      "     4.10082293e+00  1.48650608e+01]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape0] _____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.333942]]])\n",
      "_A         = array([[[-1.333942]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.333942]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.333942]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc845c99f40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.333942]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc845c99f40>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c992b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c992b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc845c99f40>\n",
      "a = NDArray([[[-1.333942]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.333942]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7fc845c99f40>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape1] _____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "_A         = array([[[ 4.68197703e-01,  2.24348330e+00,  4.72604215e-01,\n",
      "          1.18212029e-01, -6.31235898e-01,  1.17569625e+00...,  1.98646748e+00, -1.20881248e+00,\n",
      "          8.64627600e-01,  1.01337186e-03, -9.01962698e-01]]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc860019970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc860019970>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600193d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600193d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc860019970>\n",
      "a = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7fc860019970>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-2.3530958]]])\n",
      "_A         = array([[[-2.3530958]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-2.3530958]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc845c3c130>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc845c3c130>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c3c970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c3c970>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc845c3c130>\n",
      "a = NDArray([[[-2.3530958]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-2.3530958]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7fc845c3c130>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n",
      "         -0.32166547, -0.51734763],\n",
      "        [ 0.48438653,...48 ],\n",
      "        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n",
      "          0.99664706, -1.344323  ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc845c71df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc845c71df0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c71fa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c71fa0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc845c71df0>\n",
      "a = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7fc845c71df0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-1.5997132]]])\n",
      "_A         = array([[[-1.5997132]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc8e19991f0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.5997132]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc845ba18e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc845ba18e0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ba1af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ba1af0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc845ba18e0>\n",
      "a = NDArray([[[-1.5997132]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.5997132]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7fc845ba18e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "_A         = array([[[ 1.6233783 , -0.58456963,  0.86539763,  0.8150671 ,\n",
      "          0.52470696, -2.4929152 ],\n",
      "        [ 0.28299394,...27 ],\n",
      "        [ 0.07837614, -0.39979827, -0.04655599, -1.3993185 ,\n",
      "          2.621195  , -0.13064201]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc8e19991f0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc86757df40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc86757df40>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86757df70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86757df70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc86757df40>\n",
      "a = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "self       = <needle.ops.Tanh object at 0x7fc86757df40>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-1.2775043]]])\n",
      "_A         = array([[[-1.2775043]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc8e19991f0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-1.2775043]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc845b89f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc845b89f10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b89d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b89d90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc845b89f10>\n",
      "a = NDArray([[[-1.2775043]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.2775043]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7fc845b89f10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "_A         = array([[[-0.71145284,  0.96028835, -0.01700081,  0.18537076,\n",
      "         -0.92995864,  1.2871389 ],\n",
      "        [-0.69594264,...502],\n",
      "        [-0.03949696, -0.9310424 , -0.55855966, -1.2472047 ,\n",
      "         -1.1149031 , -0.5235176 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc8e19991f0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:454: in tanh\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tanh()(a)\n",
      "        a          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        self       = <needle.ops.Tanh object at 0x7fc86001a610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        op         = <needle.ops.Tanh object at 0x7fc86001a610>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86001a100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86001a100>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc86001a610>\n",
      "a = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "self       = <needle.ops.Tanh object at 0x7fc86001a610>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "A_t        = [tensor([[ 0.9080,  1.1915,  0.6277,  1.7709,  2.5803],\n",
      "        [ 1.0994, -0.2561, -0.3035, -1.2755,  1.4340],\n",
      "       ....9323],\n",
      "        [-0.1727, -0.3860,  0.6065,  0.3832, -1.2306],\n",
      "        [-0.7605,  1.3836, -1.3960, -0.2341,  0.9361]])]\n",
      "_A         = [array([[ 0.90795034,  1.1914672 ,  0.6276671 ,  1.7709465 ,  2.5802875 ],\n",
      "       [ 1.0994289 , -0.25607443, -0.303511...8323304, -1.2305677 ],\n",
      "       [-0.76054263,  1.3835862 , -1.396009  , -0.23408563,  0.9360824 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845a6ea60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845a6ea60>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a6ed00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a6ed00>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845a6ea60>\n",
      "args = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "self       = <needle.ops.Stack object at 0x7fc845a6ea60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "A_t        = [tensor([[ 0.2389,  1.7993,  0.1729, -0.1885,  1.6104],\n",
      "        [-0.5798, -1.7727,  0.7770, -0.6439,  0.3592],\n",
      "       ....3381],\n",
      "        [-1.3337, -0.5883, -1.3194, -2.0161,  0.7420],\n",
      "        [-1.2923,  0.1274, -1.2252,  1.4211,  1.0188]])]\n",
      "_A         = [array([[ 0.23885646,  1.7992946 ,  0.17290778, -0.18845753,  1.6103987 ],\n",
      "       [-0.57978904, -1.7727456 ,  0.776984...160983 ,  0.74200845],\n",
      "       [-1.2923242 ,  0.1273649 , -1.225224  ,  1.421074  ,  1.0188165 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc86008cfd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc86008cfd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86008c1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86008c1f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc86008cfd0>\n",
      "args = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7fc86008cfd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "A_t        = [tensor([[[-1.3506, -1.0219, -0.2898, -0.5111, -1.0806, -0.1566, -0.1900],\n",
      "         [-1.5699,  0.1507, -1.5068,  0.479...2785,  0.1121, -0.2383,  1.0998, -0.0714],\n",
      "         [ 0.6218,  0.6492,  1.0329, -0.8242, -0.0909,  0.3844,  0.3634]]])]\n",
      "_A         = [array([[[-1.3505836 , -1.0219276 , -0.28979254, -0.5110697 ,\n",
      "         -1.0806233 , -0.156554  , -0.18995644],\n",
      "       ...[ 0.62180406,  0.64916927,  1.0328673 , -0.82423985,\n",
      "         -0.09091089,  0.3844313 ,  0.36341754]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845a9c220>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845a9c220>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a9cf70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a9cf70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845a9c220>\n",
      "args = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7fc845a9c220>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "A_t        = [tensor([[-0.8576, -0.0482,  0.2440,  1.8527,  0.0825],\n",
      "        [-1.3493,  0.7654, -1.0452,  1.2154, -1.8788],\n",
      "       ....7364],\n",
      "        [ 0.6130,  0.6366, -0.1788,  1.4526,  0.0686],\n",
      "        [-0.0181, -0.1486, -1.0339, -1.0570, -0.2119]])]\n",
      "_A         = [array([[-0.85764205, -0.04821775,  0.24398687,  1.8527063 ,  0.08247776],\n",
      "       [-1.3493153 ,  0.7653674 , -1.045237...526075 ,  0.06863618],\n",
      "       [-0.0180711 , -0.14859328, -1.0338513 , -1.0570263 , -0.2118788 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845c81610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845c81610>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c81310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c81310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845c81610>\n",
      "args = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "self       = <needle.ops.Stack object at 0x7fc845c81610>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "A_t        = [tensor([[-0.0361,  1.2204,  0.7632, -0.0202,  2.2113],\n",
      "        [-0.4971,  1.6844, -0.6964,  0.4664,  0.3781],\n",
      "       ....2980],\n",
      "        [ 0.0499,  1.2440, -1.3311,  0.1839, -0.7144],\n",
      "        [-1.1293, -1.0743,  0.6784,  0.4646,  1.6899]])]\n",
      "_A         = [array([[-0.03613054,  1.2204468 ,  0.7631518 , -0.02020102,  2.2112603 ],\n",
      "       [-0.49705917,  1.6843735 , -0.696395...8387789, -0.7143793 ],\n",
      "       [-1.1292638 , -1.0743438 ,  0.6784423 ,  0.46458447,  1.6899037 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845c8f670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845c8f670>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c8f4f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c8f4f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845c8f670>\n",
      "args = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7fc845c8f670>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "A_t        = [tensor([[[-5.5682e-01, -8.2674e-01, -9.5333e-01, -1.0309e-01, -4.2893e-01,\n",
      "           5.5746e-04,  1.0586e+00],\n",
      "     ...02],\n",
      "         [ 1.1807e+00, -5.7011e-01, -1.6425e-01,  1.0571e-01, -1.1592e-02,\n",
      "           6.3662e-02,  3.1759e-01]]])]\n",
      "_A         = [array([[[-5.56821287e-01, -8.26741934e-01, -9.53326285e-01,\n",
      "         -1.03087865e-01, -4.28928941e-01,  5.57456922e-0...011104e-01, -1.6424966e-01,  1.0570560e-01,\n",
      "         -1.1592215e-02,  6.3661553e-02,  3.1758705e-01]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845c866a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845c866a0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c866d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c866d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845c866a0>\n",
      "args = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7fc845c866a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "A_t        = [tensor([[-0.9027, -0.7916,  2.7447,  1.3618, -0.8795],\n",
      "        [-0.8366, -1.3726,  0.8278, -0.1998, -1.3305],\n",
      "       ...0961, -0.1999, -0.4484,  1.0540,  0.0305],\n",
      "        [-0.4769,  1.5635, -1.0746,  0.9731, -1.0777]], requires_grad=True)]\n",
      "_A         = [array([[-0.9026891 , -0.7916189 ,  2.7447178 ,  1.3617536 , -0.8795233 ],\n",
      "       [-0.8365751 , -1.3725866 ,  0.827849...540462 ,  0.03045906],\n",
      "       [-0.4768823 ,  1.5634812 , -1.0746397 ,  0.9731278 , -1.0777118 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845c8f820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845c8f820>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b73310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b73310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845c8f820>\n",
      "args = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "self       = <needle.ops.Stack object at 0x7fc845c8f820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "A_t        = [tensor([[-1.1209, -0.4109, -0.1834,  1.0602,  0.6528],\n",
      "        [ 0.5972,  0.9693,  1.1120, -0.5864,  0.0547],\n",
      "       ...3290,  0.2979,  0.3833,  0.1805,  0.1305],\n",
      "        [ 0.8502,  0.6713,  1.1949, -0.3069, -0.4117]], requires_grad=True)]\n",
      "_A         = [array([[-1.1208949 , -0.41087958, -0.18338212,  1.0601658 ,  0.6527857 ],\n",
      "       [ 0.5972073 ,  0.9692809 ,  1.112041...8051435,  0.1305337 ],\n",
      "       [ 0.8501626 ,  0.6712771 ,  1.1948769 , -0.30687174, -0.41171578]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845a84490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845a84490>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a84fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a84fd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845a84490>\n",
      "args = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7fc845a84490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "A_t        = [tensor([[[-0.2897,  0.5881,  1.2632,  1.8495, -1.3362, -0.2293,  1.6220],\n",
      "         [ 0.2332,  0.4612, -1.1698,  1.140...0655, -0.2649],\n",
      "         [-2.1368, -0.2813,  0.5420, -0.8280, -0.1952,  1.2885,  1.6259]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.28966966,  0.5881318 ,  1.2632097 ,  1.8494787 ,\n",
      "         -1.3362346 , -0.22933096,  1.6220489 ],\n",
      "       ...[-2.136809  , -0.28127787,  0.54197794, -0.8279596 ,\n",
      "         -0.19517688,  1.2884692 ,  1.625865  ]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc860052b20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc860052b20>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600522b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600522b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc860052b20>\n",
      "args = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "self       = <needle.ops.Stack object at 0x7fc860052b20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "A_t        = [tensor([[-1.3193,  0.6103, -1.2339, -0.6026,  0.8272],\n",
      "        [ 1.3493,  0.1192, -0.3294,  0.6092,  1.7408],\n",
      "       ...5734, -0.6012, -0.2230,  0.0861,  0.1369],\n",
      "        [ 1.5095, -0.7127, -0.7338, -1.1747, -0.3218]], requires_grad=True)]\n",
      "_A         = [array([[-1.3193388 ,  0.6103235 , -1.2338529 , -0.6025919 ,  0.82718664],\n",
      "       [ 1.349268  ,  0.11915252, -0.329409...8610313,  0.13694113],\n",
      "       [ 1.5095038 , -0.7126971 , -0.73380625, -1.1747231 , -0.3217507 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845a6c490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845a6c490>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a6cfa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a6cfa0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845a6c490>\n",
      "args = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "self       = <needle.ops.Stack object at 0x7fc845a6c490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "A_t        = [tensor([[ 0.8628, -1.1587, -1.0618,  0.0683,  0.4480],\n",
      "        [-1.6560,  0.3594,  0.5834, -2.7582,  0.1567],\n",
      "       ...2584,  0.9400,  1.4049,  2.0662,  1.9755],\n",
      "        [ 0.7066, -1.1856, -1.0203, -0.1288, -0.1320]], requires_grad=True)]\n",
      "_A         = [array([[ 0.86278856, -1.1587424 , -1.0617669 ,  0.06834911,  0.44795617],\n",
      "       [-1.6559892 ,  0.35942334,  0.583443...662467 ,  1.975455  ],\n",
      "       [ 0.70656043, -1.1856185 , -1.0202792 , -0.12881304, -0.13199753]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc8600939d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc8600939d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600932b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600932b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc8600939d0>\n",
      "args = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7fc8600939d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "A_t        = [tensor([[[-0.5643,  0.2627, -0.2986,  2.1484,  2.2822,  2.0546, -0.5192],\n",
      "         [ 0.5852,  0.2980, -0.4843, -0.240...2493, -1.1226],\n",
      "         [ 1.3219, -0.1445, -0.9866,  0.2616,  0.9347, -0.3751,  0.2546]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.56430817,  0.26270753, -0.29860306,  2.1484196 ,\n",
      "          2.282201  ,  2.0546498 , -0.51918304],\n",
      "       ...[ 1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ,\n",
      "          0.93468   , -0.37505797,  0.25460657]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:480: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc845a8ee20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc845a8ee20>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a8e6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a8e6a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Stack object at 0x7fc845a8ee20>\n",
      "args = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "self       = <needle.ops.Stack object at 0x7fc845a8ee20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:469: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_summation[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.284491]]])\n",
      "_A         = array([[[0.284491]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.284491]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.284491]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c0e640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.284491]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845c0e640>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0e370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0e370>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[0.284491]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c0e640>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[0.284491]]], device=cpu())\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.284491]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:538: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.compact().reshape((\u001b[94m1\u001b[39;49;00m,) * (\u001b[96mself\u001b[39;49;00m.ndim - \u001b[94m1\u001b[39;49;00m) + (prod(\u001b[96mself\u001b[39;49;00m.shape),))\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.284491]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[0.284491]]], device=cpu()), new_shape = (1, 1, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1, 1, 1)\n",
      "self       = NDArray([[[0.284491]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]])\n",
      "_A         = array([[ 0.7043908 ,  1.2221886 ,  1.5080935 ],\n",
      "       [-0.585842  ,  0.04551945,  1.1883969 ],\n",
      "       [ 0.35897374,  ...4381 ],\n",
      "       [-0.14003062, -0.20621192, -0.23054962],\n",
      "       [-2.1563048 , -1.2538761 ,  1.090643  ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b9ac10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845b9ac10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b9afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b9afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b9ac10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "new_axes = (1, 0)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (1, 0)\n",
      "self       = NDArray([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0....4]\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]])\n",
      "_A         = array([[[-0.51554435, -1.0567564 ],\n",
      "        [-0.30265552,  0.4281822 ],\n",
      "        [-0.68831074,  1.6516227 ]],\n",
      "\n",
      "       [...  [[ 0.68060917,  0.5635482 ],\n",
      "        [ 1.0085729 ,  0.75971615],\n",
      "        [ 0.31350297, -2.0439312 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0....4]\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0...\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845bcc310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0...\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845bcc310>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcc970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcc970>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845bcc310>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "new_axes = (0, 2, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1)\n",
      "self       = NDArray([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0.276385...76  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0....3]\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]])\n",
      "_A         = array([[[-0.09097444,  0.11011965],\n",
      "        [-0.23238643, -1.2567368 ],\n",
      "        [-0.00447594,  0.03316377]],\n",
      "\n",
      "       [...  [[ 0.4651775 , -0.96335906],\n",
      "        [-0.22114491,  1.0182978 ],\n",
      "        [ 0.30359092, -1.609142  ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0....3]\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0...\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc86002e460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0...\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc86002e460>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86002e070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86002e070>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc86002e460>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "new_axes = (0, 1, 2)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 1, 2)\n",
      "self       = NDArray([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0.131353...551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_summation[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.87107515]]])\n",
      "_A         = array([[[0.87107515]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.87107515]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.87107515]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845a9c880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.87107515]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845a9c880>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a9cc40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845a9cc40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[0.87107515]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845a9c880>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[0.87107515]]], device=cuda())\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.87107515]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:538: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.compact().reshape((\u001b[94m1\u001b[39;49;00m,) * (\u001b[96mself\u001b[39;49;00m.ndim - \u001b[94m1\u001b[39;49;00m) + (prod(\u001b[96mself\u001b[39;49;00m.shape),))\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.87107515]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[0.87107515]]], device=cuda()), new_shape = (1, 1, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1, 1, 1)\n",
      "self       = NDArray([[[0.87107515]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]])\n",
      "_A         = array([[ 0.19680066, -0.31015334,  0.9066847 ],\n",
      "       [-0.15136234,  0.8248054 ,  0.588381  ],\n",
      "       [ 1.102897  ,  ...67875],\n",
      "       [ 0.49103832,  0.46082434,  0.2423399 ],\n",
      "       [ 2.015139  ,  0.10407976,  0.60018444]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b99070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845b99070>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b996d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b996d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b99070>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "new_axes = (1, 0)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (1, 0)\n",
      "self       = NDArray([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.... ]\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]])\n",
      "_A         = array([[[-0.3742769 , -0.15478124],\n",
      "        [-0.5490968 ,  0.1884813 ],\n",
      "        [-0.6726331 , -0.5823771 ]],\n",
      "\n",
      "       [...  [[ 0.72896844, -1.4150094 ],\n",
      "        [ 0.39689642, -2.1479504 ],\n",
      "        [ 0.84768903, -0.5966049 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.... ]\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0...\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c08f70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0...\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845c08f70>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c08190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c08190>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c08f70>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "new_axes = (0, 2, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1)\n",
      "self       = NDArray([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.516130...2   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1....4]\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]])\n",
      "_A         = array([[[-0.43343157, -0.5803195 ],\n",
      "        [-0.79609615,  0.6915893 ],\n",
      "        [ 0.4549692 ,  0.6378628 ]],\n",
      "\n",
      "       [...  [[ 1.8298346 , -0.45180562],\n",
      "        [-0.16761442,  0.9780772 ],\n",
      "        [ 0.9505592 ,  0.0114131 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1....4]\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1...\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc860026670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1...\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc860026670>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc860026e80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc860026e80>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc860026670>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "new_axes = (0, 1, 2)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 1, 2)\n",
      "self       = NDArray([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1.329009...07  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________ test_summation_backward[cpu-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[0.25179562]]])\n",
      "_A         = array([[[0.25179562]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[0.25179562]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': None}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[0.25179562]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.25179562]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c0f1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.25179562]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845c0f1f0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0fb80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0fb80>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[0.25179562]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c0f1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[0.25179562]]], device=cpu())\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.25179562]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:538: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.compact().reshape((\u001b[94m1\u001b[39;49;00m,) * (\u001b[96mself\u001b[39;49;00m.ndim - \u001b[94m1\u001b[39;49;00m) + (prod(\u001b[96mself\u001b[39;49;00m.shape),))\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[0.25179562]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[0.25179562]]], device=cpu()), new_shape = (1, 1, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1, 1, 1)\n",
      "self       = NDArray([[[0.25179562]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape1-0] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]])\n",
      "_A         = array([[-0.26194048,  0.36832854,  0.524743  ],\n",
      "       [ 0.72734344, -0.6690794 ,  0.2118702 ],\n",
      "       [-0.17624503, -...3194 ],\n",
      "       [ 0.21689712,  0.43637753, -0.49504337],\n",
      "       [ 1.4166641 , -0.2632424 ,  1.5031598 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b0c9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845b0c9a0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b0cb20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b0cb20>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b0c9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "new_axes = (1, 0)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (1, 0)\n",
      "self       = NDArray([[-0.26194048  0.36832854  0.524743  ]\n",
      " [ 0.72734344 -0.6690794   0.2118702 ]\n",
      " [-0.17624503 -0.5157768  -1.9483194 ]\n",
      " [ 0.21689712  0.43637753 -0.49504337]\n",
      " [ 1.4166641  -0.2632424   1.5031598 ]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape2-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1....7]\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]])\n",
      "_A         = array([[[-1.2881242 , -0.24279231],\n",
      "        [ 0.5133875 ,  1.1767663 ],\n",
      "        [-1.223357  ,  1.9838535 ]],\n",
      "\n",
      "       [...  [[-0.17416234,  0.21612613],\n",
      "        [ 0.66463155,  0.21836282],\n",
      "        [ 0.6723505 , -0.5805451 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1....7]\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b8aee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1...\n",
      "  [-1.1708896  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845b8aee0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b8a1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b8a1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b8aee0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "new_axes = (0, 2, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1)\n",
      "self       = NDArray([[[-1.2881242  -0.24279231]\n",
      "  [ 0.5133875   1.1767663 ]\n",
      "  [-1.223357    1.9838535 ]]\n",
      "\n",
      " [[-0.40351707  1.587832...96  -1.0046583 ]]\n",
      "\n",
      " [[-0.17416234  0.21612613]\n",
      "  [ 0.66463155  0.21836282]\n",
      "  [ 0.6723505  -0.5805451 ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape3-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.... ]\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]])\n",
      "_A         = array([[[ 0.94992834, -0.13286018],\n",
      "        [-0.05775288,  2.3362038 ],\n",
      "        [-0.02063232,  0.17829332]],\n",
      "\n",
      "       [...  [[-0.04282787,  1.6461416 ],\n",
      "        [ 0.21175696, -0.98908705],\n",
      "        [-0.79134655,  1.2193956 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.... ]\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845ce9160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1...\n",
      "  [-1.2121632  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845ce9160>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ce9ee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ce9ee0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845ce9160>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "new_axes = (0, 1, 2)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 1, 2)\n",
      "self       = NDArray([[[ 0.94992834 -0.13286018]\n",
      "  [-0.05775288  2.3362038 ]\n",
      "  [-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755...32  -0.9914862 ]]\n",
      "\n",
      " [[-0.04282787  1.6461416 ]\n",
      "  [ 0.21175696 -0.98908705]\n",
      "  [-0.79134655  1.2193956 ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[1.2554915]]])\n",
      "_A         = array([[[1.2554915]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[1.2554915]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': None}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[1.2554915]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[1.2554915]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c89760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[1.2554915]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845c89760>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c895b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c895b0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[1.2554915]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845c89760>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[1.2554915]]], device=cuda())\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[1.2554915]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:538: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.compact().reshape((\u001b[94m1\u001b[39;49;00m,) * (\u001b[96mself\u001b[39;49;00m.ndim - \u001b[94m1\u001b[39;49;00m) + (prod(\u001b[96mself\u001b[39;49;00m.shape),))\n",
      "        axis       = None\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[1.2554915]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[1.2554915]]], device=cuda()), new_shape = (1, 1, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1, 1, 1)\n",
      "self       = NDArray([[[1.2554915]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]])\n",
      "_A         = array([[-0.01285293,  0.494915  , -0.7648952 ],\n",
      "       [-0.33858636,  1.2390388 , -1.1854753 ],\n",
      "       [ 0.45743605,  ...21282],\n",
      "       [ 1.6258224 ,  0.65709186,  0.06870755],\n",
      "       [-1.2057985 , -1.0558244 ,  1.0220021 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845bcfa90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845bcfa90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcf5b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcf5b0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845bcfa90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 0\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "new_axes = (1, 0)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (1, 0)\n",
      "self       = NDArray([[-0.01285293  0.494915   -0.7648952 ]\n",
      " [-0.33858636  1.2390388  -1.1854753 ]\n",
      " [ 0.45743605  1.3178569   0.32221282]\n",
      " [ 1.6258224   0.65709186  0.06870755]\n",
      " [-1.2057985  -1.0558244   1.0220021 ]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.... ]\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]])\n",
      "_A         = array([[[-0.20333268,  0.9668824 ],\n",
      "        [-0.25639066,  0.5157999 ],\n",
      "        [-0.44359782,  0.83550507]],\n",
      "\n",
      "       [...  [[ 0.3785013 , -1.1597295 ],\n",
      "        [ 0.8766111 , -1.7572289 ],\n",
      "        [ 0.31880176, -1.6526436 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.... ]\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc860019a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1...\n",
      "  [-0.20674394  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc860019a90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc860019d00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc860019d00>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc860019a90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 1\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "new_axes = (0, 2, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 2, 1)\n",
      "self       = NDArray([[[-0.20333268  0.9668824 ]\n",
      "  [-0.25639066  0.5157999 ]\n",
      "  [-0.44359782  0.83550507]]\n",
      "\n",
      " [[-0.7541298  -1.349219...94  1.478983  ]]\n",
      "\n",
      " [[ 0.3785013  -1.1597295 ]\n",
      "  [ 0.8766111  -1.7572289 ]\n",
      "  [ 0.31880176 -1.6526436 ]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "_A         = array([[[ 3.1887993e-01,  1.5159522e-03],\n",
      "        [-9.1810602e-01,  8.5386848e-01],\n",
      "        [ 5.3351974e-01,  6.397005...,  3.2395300e-01],\n",
      "        [ 2.3655061e-01,  2.1437683e+00],\n",
      "        [ 1.1814048e+00, -1.3010720e+00]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc8e19964c0>\n",
      "        kwargs     = {'axes': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:308: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\n",
      "        a          = needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b9ab20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54880e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        op         = <needle.ops.Summation object at 0x7fc845b9ab20>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b9a0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845b9a0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:297: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.summation(a, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "        self       = <needle.ops.Summation object at 0x7fc845b9ab20>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in summation\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.sum(axis=axis, keepdims=keepdims)\n",
      "        a          = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:559: in sum\n",
      "    view, out = \u001b[96mself\u001b[39;49;00m.reduce_view_out(axis, keepdims=keepdims)\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:547: in reduce_view_out\n",
      "    view = \u001b[96mself\u001b[39;49;00m.permute(\n",
      "        axis       = 2\n",
      "        keepdims   = False\n",
      "        self       = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "new_axes = (0, 1, 2)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpermute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_axes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Permute order of the dimensions.  new_axes describes a permuation of the\u001b[39;49;00m\n",
      "    \u001b[33m    existing axes, so e.g.:\u001b[39;49;00m\n",
      "    \u001b[33m      - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\u001b[39;49;00m\n",
      "    \u001b[33m        would convert this to \"BCHW\" order.\u001b[39;49;00m\n",
      "    \u001b[33m      - For a 2D array, .permute((1,0)) would transpose the array.\u001b[39;49;00m\n",
      "    \u001b[33m    Like reshape, this operation should not copy memory, but achieves the\u001b[39;49;00m\n",
      "    \u001b[33m    permuting by just adjusting the shape/strides of the array.  That is,\u001b[39;49;00m\n",
      "    \u001b[33m    it returns a new array that has the dimensions permuted as desired, but\u001b[39;49;00m\n",
      "    \u001b[33m    which points to the same memroy as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_axes (tuple): permuation order of the dimensions\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDarray : new NDArray object with permuted dimensions, pointing\u001b[39;49;00m\n",
      "    \u001b[33m        to the same memory as the original NDArray (i.e., just shape and\u001b[39;49;00m\n",
      "    \u001b[33m        strides changed).\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_axes   = (0, 1, 2)\n",
      "self       = NDArray([[[ 3.1887993e-01  1.5159522e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      " [[-3.... [[ 1.4263909e+00  3.2395300e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:274: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________ test_broadcast_to[cpu-shape0-shape_to0] ____________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), shape_to = (3, 3, 3), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.43455654]]])\n",
      "_A         = array([[[-0.43455654]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "shape_to   = (3, 3, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:201: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:265: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\n",
      "        a          = needle.Tensor([[[-0.43455654]]])\n",
      "        shape      = (3, 3, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.43455654]]]),)\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845c0efa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.43455654]]]),)\n",
      "        op         = <needle.ops.BroadcastTo object at 0x7fc845c0efa0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0eaf0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c0eaf0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape).compact()\n",
      "        a          = NDArray([[[-0.43455654]]], device=cpu())\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845c0efa0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:609: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.broadcast_to(new_shape)\n",
      "        array      = NDArray([[[-0.43455654]]], device=cpu())\n",
      "        new_shape  = (3, 3, 3)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.43455654]]], device=cpu()), new_shape = (3, 3, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mbroadcast_to\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Broadcast an array to a new shape.  new_shape's elements must be the\u001b[39;49;00m\n",
      "    \u001b[33m    same as the original shape, except for dimensions in the self where\u001b[39;49;00m\n",
      "    \u001b[33m    the size = 1 (which can then be broadcast to any size).  As with the\u001b[39;49;00m\n",
      "    \u001b[33m    previous calls, this will not copy memory, and just achieves\u001b[39;49;00m\n",
      "    \u001b[33m    broadcasting by manipulating the strides.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        assertion error if new_shape[i] != shape[i] for all i where\u001b[39;49;00m\n",
      "    \u001b[33m        shape[i] != 1\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): shape to broadcast to\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray: the new NDArray object with the new broadcast shape; should\u001b[39;49;00m\n",
      "    \u001b[33m        point to the same memory as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (3, 3, 3)\n",
      "self       = NDArray([[[-0.43455654]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:297: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________ test_broadcast_to[cpu-shape1-shape_to1] ____________________\u001b[0m\n",
      "\n",
      "shape = (4, 1, 6), shape_to = (4, 3, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.59180....04162927 -1.2710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]])\n",
      "_A         = array([[[ 0.550488  ,  0.06143114,  1.2069732 ,  0.2528209 ,\n",
      "         -0.36453617, -0.1625579 ]],\n",
      "\n",
      "       [[ 2.2881606...1]],\n",
      "\n",
      "       [[ 0.68353325, -1.1112776 , -2.5426693 ,  0.51752055,\n",
      "          0.4700926 ,  0.36956707]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 1, 6)\n",
      "shape_to   = (4, 3, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:201: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:265: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\n",
      "        a          = needle.Tensor([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.59180....04162927 -1.2710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]])\n",
      "        shape      = (4, 3, 6)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918...4162927 -1.2710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]]),)\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845cd1f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918...4162927 -1.2710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]]),)\n",
      "        op         = <needle.ops.BroadcastTo object at 0x7fc845cd1f10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845cd10d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845cd10d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape).compact()\n",
      "        a          = NDArray([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918057  -1...710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]], device=cpu())\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845cd1f10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:609: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.broadcast_to(new_shape)\n",
      "        array      = NDArray([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918057  -1...710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]], device=cpu())\n",
      "        new_shape  = (4, 3, 6)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918057  -1...710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]], device=cpu())\n",
      "new_shape = (4, 3, 6)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mbroadcast_to\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Broadcast an array to a new shape.  new_shape's elements must be the\u001b[39;49;00m\n",
      "    \u001b[33m    same as the original shape, except for dimensions in the self where\u001b[39;49;00m\n",
      "    \u001b[33m    the size = 1 (which can then be broadcast to any size).  As with the\u001b[39;49;00m\n",
      "    \u001b[33m    previous calls, this will not copy memory, and just achieves\u001b[39;49;00m\n",
      "    \u001b[33m    broadcasting by manipulating the strides.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        assertion error if new_shape[i] != shape[i] for all i where\u001b[39;49;00m\n",
      "    \u001b[33m        shape[i] != 1\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): shape to broadcast to\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray: the new NDArray object with the new broadcast shape; should\u001b[39;49;00m\n",
      "    \u001b[33m        point to the same memory as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (4, 3, 6)\n",
      "self       = NDArray([[[ 0.550488    0.06143114  1.2069732   0.2528209  -0.36453617\n",
      "   -0.1625579 ]]\n",
      "\n",
      " [[ 2.2881606   1.5918057  -1...710459\n",
      "    0.00565341]]\n",
      "\n",
      " [[ 0.68353325 -1.1112776  -2.5426693   0.51752055  0.4700926\n",
      "    0.36956707]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:297: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape0-shape_to0] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), shape_to = (3, 3, 3), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.42903188]]])\n",
      "_A         = array([[[-0.42903188]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "shape_to   = (3, 3, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:201: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:265: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\n",
      "        a          = needle.Tensor([[[-0.42903188]]])\n",
      "        shape      = (3, 3, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.42903188]]]),)\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845ba06a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.42903188]]]),)\n",
      "        op         = <needle.ops.BroadcastTo object at 0x7fc845ba06a0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ba0c40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845ba0c40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape).compact()\n",
      "        a          = NDArray([[[-0.42903188]]], device=cuda())\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845ba06a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:609: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.broadcast_to(new_shape)\n",
      "        array      = NDArray([[[-0.42903188]]], device=cuda())\n",
      "        new_shape  = (3, 3, 3)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.42903188]]], device=cuda()), new_shape = (3, 3, 3)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mbroadcast_to\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Broadcast an array to a new shape.  new_shape's elements must be the\u001b[39;49;00m\n",
      "    \u001b[33m    same as the original shape, except for dimensions in the self where\u001b[39;49;00m\n",
      "    \u001b[33m    the size = 1 (which can then be broadcast to any size).  As with the\u001b[39;49;00m\n",
      "    \u001b[33m    previous calls, this will not copy memory, and just achieves\u001b[39;49;00m\n",
      "    \u001b[33m    broadcasting by manipulating the strides.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        assertion error if new_shape[i] != shape[i] for all i where\u001b[39;49;00m\n",
      "    \u001b[33m        shape[i] != 1\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): shape to broadcast to\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray: the new NDArray object with the new broadcast shape; should\u001b[39;49;00m\n",
      "    \u001b[33m        point to the same memory as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (3, 3, 3)\n",
      "self       = NDArray([[[-0.42903188]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:297: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape1-shape_to1] ___________________\u001b[0m\n",
      "\n",
      "shape = (4, 1, 6), shape_to = (4, 3, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889...47289303  1.3856629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]])\n",
      "_A         = array([[[-0.46139053,  0.24671845,  1.2265509 ,  0.4308405 ,\n",
      "          0.54698414,  0.00562425]],\n",
      "\n",
      "       [[-0.0952589... ]],\n",
      "\n",
      "       [[-0.23845685,  0.37773255,  0.9394455 ,  2.1831763 ,\n",
      "         -0.13268672,  1.4607326 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 1, 6)\n",
      "shape_to   = (4, 3, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:201: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:265: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\n",
      "        a          = needle.Tensor([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889...47289303  1.3856629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]])\n",
      "        shape      = (4, 3, 6)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.6688...289303  1.3856629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]]),)\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845c777f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.6688...289303  1.3856629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]]),)\n",
      "        op         = <needle.ops.BroadcastTo object at 0x7fc845c777f0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c77820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845c77820>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape).compact()\n",
      "        a          = NDArray([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889954 -1...6629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]], device=cuda())\n",
      "        self       = <needle.ops.BroadcastTo object at 0x7fc845c777f0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:609: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.broadcast_to(new_shape)\n",
      "        array      = NDArray([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889954 -1...6629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]], device=cuda())\n",
      "        new_shape  = (4, 3, 6)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889954 -1...6629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]], device=cuda())\n",
      "new_shape = (4, 3, 6)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mbroadcast_to\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Broadcast an array to a new shape.  new_shape's elements must be the\u001b[39;49;00m\n",
      "    \u001b[33m    same as the original shape, except for dimensions in the self where\u001b[39;49;00m\n",
      "    \u001b[33m    the size = 1 (which can then be broadcast to any size).  As with the\u001b[39;49;00m\n",
      "    \u001b[33m    previous calls, this will not copy memory, and just achieves\u001b[39;49;00m\n",
      "    \u001b[33m    broadcasting by manipulating the strides.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        assertion error if new_shape[i] != shape[i] for all i where\u001b[39;49;00m\n",
      "    \u001b[33m        shape[i] != 1\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): shape to broadcast to\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray: the new NDArray object with the new broadcast shape; should\u001b[39;49;00m\n",
      "    \u001b[33m        point to the same memory as the original array.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (4, 3, 6)\n",
      "self       = NDArray([[[-0.46139053  0.24671845  1.2265509   0.4308405   0.54698414\n",
      "    0.00562425]]\n",
      "\n",
      " [[-0.09525896  0.66889954 -1...6629\n",
      "    1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0.37773255  0.9394455   2.1831763  -0.13268672\n",
      "    1.4607326 ]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:297: NotImplementedError\n",
      "\u001b[31m\u001b[1m______________________ test_reshape[cpu-shape0-shape_to0] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), shape_to = (1,), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.28773278]]])\n",
      "_A         = array([[[0.28773278]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "shape_to   = (1,)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:211: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:239: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\n",
      "        a          = needle.Tensor([[[0.28773278]]])\n",
      "        shape      = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.28773278]]]),)\n",
      "        self       = <needle.ops.Reshape object at 0x7fc86008c400>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.28773278]]]),)\n",
      "        op         = <needle.ops.Reshape object at 0x7fc86008c400>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcf850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcf850>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:229: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "        a          = NDArray([[[0.28773278]]], device=cpu())\n",
      "        self       = <needle.ops.Reshape object at 0x7fc86008c400>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:613: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.reshape(new_shape)\n",
      "        array      = NDArray([[[0.28773278]]], device=cpu())\n",
      "        new_shape  = (1,)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[0.28773278]]], device=cpu()), new_shape = (1,)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1,)\n",
      "self       = NDArray([[[0.28773278]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m______________________ test_reshape[cpu-shape1-shape_to1] ______________________\u001b[0m\n",
      "\n",
      "shape = (4, 1, 6), shape_to = (6, 4, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.259436...74093074 -0.50888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]])\n",
      "_A         = array([[[-0.7782297 , -1.108506  ,  1.7799296 ,  2.1140475 ,\n",
      "         -0.9384094 , -0.2541468 ]],\n",
      "\n",
      "       [[ 1.4692454... ]],\n",
      "\n",
      "       [[ 0.53634965, -2.1118162 , -0.27859452,  1.0560247 ,\n",
      "         -0.5948552 , -1.2022481 ]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 1, 6)\n",
      "shape_to   = (6, 4, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:211: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:239: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\n",
      "        a          = needle.Tensor([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.259436...74093074 -0.50888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]])\n",
      "        shape      = (6, 4, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.25943...093074 -0.50888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]]),)\n",
      "        self       = <needle.ops.Reshape object at 0x7fc860019fa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.25943...093074 -0.50888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]]),)\n",
      "        op         = <needle.ops.Reshape object at 0x7fc860019fa0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600193d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc8600193d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:229: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "        a          = NDArray([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.2594365   1....888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]], device=cpu())\n",
      "        self       = <needle.ops.Reshape object at 0x7fc860019fa0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:613: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.reshape(new_shape)\n",
      "        array      = NDArray([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.2594365   1....888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]], device=cpu())\n",
      "        new_shape  = (6, 4, 1)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.2594365   1....888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]], device=cpu())\n",
      "new_shape = (6, 4, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (6, 4, 1)\n",
      "self       = NDArray([[[-0.7782297  -1.108506    1.7799296   2.1140475  -0.9384094\n",
      "   -0.2541468 ]]\n",
      "\n",
      " [[ 1.4692454  -0.2594365   1....888014\n",
      "    0.408108  ]]\n",
      "\n",
      " [[ 0.53634965 -2.1118162  -0.27859452  1.0560247  -0.5948552\n",
      "   -1.2022481 ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape0-shape_to0] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), shape_to = (1,), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.1025962]]])\n",
      "_A         = array([[[-1.1025962]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "shape_to   = (1,)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:211: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:239: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\n",
      "        a          = needle.Tensor([[[-1.1025962]]])\n",
      "        shape      = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.1025962]]]),)\n",
      "        self       = <needle.ops.Reshape object at 0x7fc86008d1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.1025962]]]),)\n",
      "        op         = <needle.ops.Reshape object at 0x7fc86008d1f0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86008d670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc86008d670>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:229: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "        a          = NDArray([[[-1.1025962]]], device=cuda())\n",
      "        self       = <needle.ops.Reshape object at 0x7fc86008d1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:613: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.reshape(new_shape)\n",
      "        array      = NDArray([[[-1.1025962]]], device=cuda())\n",
      "        new_shape  = (1,)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-1.1025962]]], device=cuda()), new_shape = (1,)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (1,)\n",
      "self       = NDArray([[[-1.1025962]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape1-shape_to1] ______________________\u001b[0m\n",
      "\n",
      "shape = (4, 1, 6), shape_to = (6, 4, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.18546...918218   0.53199524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]])\n",
      "_A         = array([[[-0.0601517 , -0.05933083,  1.175274  , -0.02697281,\n",
      "          0.05634464, -0.22966217]],\n",
      "\n",
      "       [[-1.2457664... ]],\n",
      "\n",
      "       [[ 0.64864075, -0.79526454, -0.11231624,  1.3642133 ,\n",
      "          0.05703132, -0.44432947]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 1, 6)\n",
      "shape_to   = (6, 4, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:211: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:239: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\n",
      "        a          = needle.Tensor([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.18546...918218   0.53199524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]])\n",
      "        shape      = (6, 4, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854...8218   0.53199524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]]),)\n",
      "        self       = <needle.ops.Reshape object at 0x7fc845bcd4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854...8218   0.53199524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]]),)\n",
      "        op         = <needle.ops.Reshape object at 0x7fc845bcd4c0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcd460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fc845bcd460>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:229: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "        a          = NDArray([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854648  -0...9524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]], device=cuda())\n",
      "        self       = <needle.ops.Reshape object at 0x7fc845bcd4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:613: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m array.reshape(new_shape)\n",
      "        array      = NDArray([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854648  -0...9524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]], device=cuda())\n",
      "        new_shape  = (6, 4, 1)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854648  -0...9524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]], device=cuda())\n",
      "new_shape = (6, 4, 1)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mreshape\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, new_shape):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Reshape the matrix without copying memory.  This will return a matrix\u001b[39;49;00m\n",
      "    \u001b[33m    that corresponds to a reshaped array but points to the same memory as\u001b[39;49;00m\n",
      "    \u001b[33m    the original array.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Raises:\u001b[39;49;00m\n",
      "    \u001b[33m        ValueError if product of current shape is not equal to the product\u001b[39;49;00m\n",
      "    \u001b[33m        of the new shape, or if the matrix is not compact.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        new_shape (tuple): new shape of the array\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        NDArray : reshaped array; this will point to thep\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "new_shape  = (6, 4, 1)\n",
      "self       = NDArray([[[-0.0601517  -0.05933083  1.175274   -0.02697281  0.05634464\n",
      "   -0.22966217]]\n",
      "\n",
      " [[-1.2457664   0.1854648  -0...9524\n",
      "    1.245671  ]]\n",
      "\n",
      " [[ 0.64864075 -0.79526454 -0.11231624  1.3642133   0.05703132\n",
      "   -0.44432947]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:249: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.76478755]]])\n",
      "_A         = array([[[-0.76478755]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.76478755]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.76478755]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c484f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.76478755]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c484f0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c48d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c48d90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c484f0>\n",
      "a = NDArray([[[-0.76478755]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.76478755]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c484f0>\n",
      "swap1      = 0\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399 ...1.4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]])\n",
      "_A         = array([[[ 1.428199  , -0.42389697,  0.8514999 ,  1.2843655 ,\n",
      "          0.4156105 ,  2.2930398 ],\n",
      "        [-0.02262507,...98 ],\n",
      "        [-0.88521427,  0.6966768 , -0.3164875 , -0.19568315,\n",
      "          0.7862357 , -0.92159903]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399 ...1.4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399...4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc86001b9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399...4833229   0.02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc86001b9d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc86001b0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc86001b0a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc86001b9d0>\n",
      "a = NDArray([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399  -0.08...02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.428199   -0.42389697  0.8514999   1.2843655   0.4156105\n",
      "    2.2930398 ]\n",
      "  [-0.02262507 -1.1791399  -0.08...02208569\n",
      "    1.2701598 ]\n",
      "  [-0.88521427  0.6966768  -0.3164875  -0.19568315  0.7862357\n",
      "   -0.92159903]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc86001b9d0>\n",
      "swap1      = 0\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.5497496]]])\n",
      "_A         = array([[[0.5497496]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.5497496]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.5497496]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc860018100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.5497496]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc860018100>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc8600187f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc8600187f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc860018100>\n",
      "a = NDArray([[[0.5497496]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.5497496]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7fc860018100>\n",
      "swap1      = 0\n",
      "swap2      = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655...0.5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]])\n",
      "_A         = array([[[ 1.1175423 ,  0.2273078 ,  0.01652352,  0.6953092 ,\n",
      "          0.37275058, -0.1833131 ],\n",
      "        [ 0.5189381 ,...12 ],\n",
      "        [-0.5111894 ,  0.16540636,  1.0318601 , -0.7140344 ,\n",
      "          0.55253035,  0.5942802 ]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655...0.5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.922165...5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c8ad90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.922165...5732212   1.6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c8ad90>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c8a370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c8a370>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c8ad90>\n",
      "a = NDArray([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655  -0.6...6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.1175423   0.2273078   0.01652352  0.6953092   0.37275058\n",
      "   -0.1833131 ]\n",
      "  [ 0.5189381   1.9221655  -0.6...6454173\n",
      "    0.3600512 ]\n",
      "  [-0.5111894   0.16540636  1.0318601  -0.7140344   0.55253035\n",
      "    0.5942802 ]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c8ad90>\n",
      "swap1      = 0\n",
      "swap2      = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.252342]]])\n",
      "_A         = array([[[0.252342]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.252342]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.252342]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845a8ec40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.252342]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845a8ec40>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845a8e490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845a8e490>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845a8ec40>\n",
      "a = NDArray([[[0.252342]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.252342]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845a8ec40>\n",
      "swap1      = 2\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308...0.18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]])\n",
      "_A         = array([[[-0.134239  ,  0.75096023, -1.4823366 , -0.40712276,\n",
      "          0.7533559 , -1.0401373 ],\n",
      "        [-0.839335  ,...27 ],\n",
      "        [ 1.20037   ,  1.4225695 ,  0.14660548, -0.4510802 ,\n",
      "          0.5955475 , -0.59707063]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308...0.18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.0386030...18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845bcd910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.0386030...18599093  0.26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845bcd910>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845bcdca0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845bcdca0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845bcd910>\n",
      "a = NDArray([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308 -0.65...26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.134239    0.75096023 -1.4823366  -0.40712276  0.7533559\n",
      "   -1.0401373 ]\n",
      "  [-0.839335    0.03860308 -0.65...26904282\n",
      "   -1.5005527 ]\n",
      "  [ 1.20037     1.4225695   0.14660548 -0.4510802   0.5955475\n",
      "   -0.59707063]]], device=cpu())\n",
      "dims       = 3\n",
      "new_axes   = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845bcd910>\n",
      "swap1      = 2\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[2.0202532]]])\n",
      "_A         = array([[[2.0202532]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[2.0202532]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[2.0202532]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c224f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[2.0202532]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c224f0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c22eb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c22eb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c224f0>\n",
      "a = NDArray([[[2.0202532]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[2.0202532]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c224f0>\n",
      "swap1      = 0\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517 ....92294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]])\n",
      "_A         = array([[[ 0.43215325,  0.45116025, -0.22739509,  1.115375  ,\n",
      "         -1.4533465 ,  0.12058777],\n",
      "        [ 0.01037335,...46 ],\n",
      "        [ 0.3929003 ,  0.03355066, -0.27658293,  1.1708506 ,\n",
      "         -0.11228849, -0.09884176]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517 ....92294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517...2294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c2b190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517...2294955  0.19945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c2b190>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c2b580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c2b580>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c2b190>\n",
      "a = NDArray([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517  -0.63...945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.43215325  0.45116025 -0.22739509  1.115375   -1.4533465\n",
      "    0.12058777]\n",
      "  [ 0.01037335 -0.8433517  -0.63...945478\n",
      "   -0.3807846 ]\n",
      "  [ 0.3929003   0.03355066 -0.27658293  1.1708506  -0.11228849\n",
      "   -0.09884176]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [1, 0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c2b190>\n",
      "swap1      = 0\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[0.14226767]]])\n",
      "_A         = array([[[0.14226767]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[0.14226767]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[0.14226767]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845cabcd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[0.14226767]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845cabcd0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845cab340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845cab340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845cabcd0>\n",
      "a = NDArray([[[0.14226767]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.14226767]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845cabcd0>\n",
      "swap1      = 0\n",
      "swap2      = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045...-0.3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]])\n",
      "_A         = array([[[-0.45924038, -1.0967047 , -0.7741067 , -1.6878831 ,\n",
      "          0.7713184 , -0.17223054],\n",
      "        [-0.6361363 ,...345],\n",
      "        [-0.15298192,  0.0542486 ,  0.12609395, -0.8680353 ,\n",
      "         -1.1219878 ,  0.32406053]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045...-0.3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.0489604....3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c3c4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.0489604....3429354  -0.3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c3c4c0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c3ca30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c3ca30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c3c4c0>\n",
      "a = NDArray([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045 -0.54...3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.45924038 -1.0967047  -0.7741067  -1.6878831   0.7713184\n",
      "   -0.17223054]\n",
      "  [-0.6361363  -0.04896045 -0.54...3446141\n",
      "   -0.79503345]\n",
      "  [-0.15298192  0.0542486   0.12609395 -0.8680353  -1.1219878\n",
      "    0.32406053]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [2, 1, 0]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c3c4c0>\n",
      "swap1      = 0\n",
      "swap2      = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.33231008]]])\n",
      "_A         = array([[[-0.33231008]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.33231008]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.33231008]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc845c846d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.33231008]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc845c846d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c847c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc845c847c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc845c846d0>\n",
      "a = NDArray([[[-0.33231008]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.33231008]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7fc845c846d0>\n",
      "swap1      = 2\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882... -1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]])\n",
      "_A         = array([[[-0.80980945, -0.58157414,  1.3430595 ,  0.14294803,\n",
      "         -0.43587744, -0.8963174 ],\n",
      "        [ 0.12310173,...281],\n",
      "        [-0.729695  , -0.27707356,  2.2894404 ,  1.4521582 ,\n",
      "         -1.4271829 ,  0.22722833]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:220: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882... -1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.572888...1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc86004d070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.572888...1.5549369  -1.361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc86004d070>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc86004d580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'transpose'\") raised in repr()] Tensor object at 0x7fc86004d580>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc86004d070>\n",
      "a = NDArray([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882   0.1....361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        dims = \u001b[96mlen\u001b[39;49;00m(a.shape)\n",
      "        (swap1, swap2) = \u001b[96mself\u001b[39;49;00m.axes \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m (dims-\u001b[94m1\u001b[39;49;00m, dims-\u001b[94m2\u001b[39;49;00m)\n",
      "        new_axes = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(dims))\n",
      "        new_axes[swap1], new_axes[swap2] = new_axes[swap2], new_axes[swap1]\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.transpose(a, axes=new_axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.80980945 -0.58157414  1.3430595   0.14294803 -0.43587744\n",
      "   -0.8963174 ]\n",
      "  [ 0.12310173  0.5728882   0.1....361934\n",
      "   -0.17819281]\n",
      "  [-0.729695   -0.27707356  2.2894404   1.4521582  -1.4271829\n",
      "    0.22722833]]], device=cuda())\n",
      "dims       = 3\n",
      "new_axes   = [0, 2, 1]\n",
      "self       = <needle.ops.Transpose object at 0x7fc86004d070>\n",
      "swap1      = 2\n",
      "swap2      = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:210: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.5344331]]])\n",
      "A_t        = tensor([[[-1.5344]]])\n",
      "_A         = array([[[-1.5344331]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.5344331]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.5344331]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc860038eb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.5344331]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc860038eb0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc8600387c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc8600387c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc860038eb0>\n",
      "Z = NDArray([[[-1.5344331]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.5344331]]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc860038eb0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]])\n",
      "A_t        = tensor([[ 0.0536,  0.7091, -0.6595],\n",
      "        [ 0.4210,  0.7163, -0.5331],\n",
      "        [ 0.7600, -0.5297,  0.1033],\n",
      "        [ 0.6210,  0.8935, -0.1603],\n",
      "        [-0.2013, -0.2559,  0.7845]])\n",
      "_A         = array([[ 0.05357093,  0.7091037 , -0.6595013 ],\n",
      "       [ 0.4209701 ,  0.7163161 , -0.53314596],\n",
      "       [ 0.75999004, -...2609 ],\n",
      "       [ 0.62098956,  0.8935139 , -0.16026992],\n",
      "       [-0.20125377, -0.25592747,  0.7845221 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845b9b3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845b9b3d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845b9baf0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845b9baf0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845b9b3d0>\n",
      "Z = NDArray([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[ 0.05357093  0.7091037  -0.6595013 ]\n",
      " [ 0.4209701   0.7163161  -0.53314596]\n",
      " [ 0.75999004 -0.52965707  0.1032609 ]\n",
      " [ 0.62098956  0.8935139  -0.16026992]\n",
      " [-0.20125377 -0.25592747  0.7845221 ]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845b9b3d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0....3]\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]])\n",
      "A_t        = tensor([[[-0.1867,  0.0383],\n",
      "         [-0.0550, -0.3202],\n",
      "         [-0.9666, -2.5009]],\n",
      "\n",
      "        [[-0.0104,  0.2827],\n",
      "...         [-1.0487,  0.9679]],\n",
      "\n",
      "        [[ 1.3501,  0.8159],\n",
      "         [-0.7212,  0.4590],\n",
      "         [ 0.2047,  2.6674]]])\n",
      "_A         = array([[[-0.18668747,  0.03825841],\n",
      "        [-0.05503474, -0.3201902 ],\n",
      "        [-0.966571  , -2.500891  ]],\n",
      "\n",
      "       [...  [[ 1.3500627 ,  0.8158523 ],\n",
      "        [-0.72117835,  0.45901817],\n",
      "        [ 0.20466456,  2.6674361 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0....3]\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0...\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845c09f70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0...\n",
      "  [-1.0486687   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845c09f70>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c09550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c09550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845c09f70>\n",
      "Z = NDArray([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0.282676...87   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-0.18668747  0.03825841]\n",
      "  [-0.05503474 -0.3201902 ]\n",
      "  [-0.966571   -2.500891  ]]\n",
      "\n",
      " [[-0.01040582  0.282676...87   0.96790123]]\n",
      "\n",
      " [[ 1.3500627   0.8158523 ]\n",
      "  [-0.72117835  0.45901817]\n",
      "  [ 0.20466456  2.6674361 ]]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845c09f70>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.... ]\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]])\n",
      "A_t        = tensor([[[-1.7313,  2.3088],\n",
      "         [ 0.1194,  0.6174],\n",
      "         [-0.3792, -0.1933]],\n",
      "\n",
      "        [[-0.2145,  0.6835],\n",
      "...         [-1.4058, -1.3156]],\n",
      "\n",
      "        [[-0.8430, -0.4802],\n",
      "         [-0.4784,  0.9575],\n",
      "         [-1.0233, -1.1449]]])\n",
      "_A         = array([[[-1.7313014 ,  2.3088462 ],\n",
      "        [ 0.11944567,  0.61742485],\n",
      "        [-0.37917662, -0.1933147 ]],\n",
      "\n",
      "       [...  [[-0.8429701 , -0.48019046],\n",
      "        [-0.47839305,  0.95748687],\n",
      "        [-1.0232737 , -1.1449004 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.... ]\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0...\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc86008ba60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0...\n",
      "  [-1.4058412  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc86008ba60>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc86008b4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc86008b4c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc86008ba60>\n",
      "Z = NDArray([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.683539...12  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.7313014   2.3088462 ]\n",
      "  [ 0.11944567  0.61742485]\n",
      "  [-0.37917662 -0.1933147 ]]\n",
      "\n",
      " [[-0.2145116   0.683539...12  -1.3155643 ]]\n",
      "\n",
      " [[-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]\n",
      "  [-1.0232737  -1.1449004 ]]], device=cpu())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc86008ba60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-1.833531]]])\n",
      "A_t        = tensor([[[-1.8335]]])\n",
      "_A         = array([[[-1.833531]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-1.833531]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-1.833531]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845ba1160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-1.833531]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845ba1160>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845ba1640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845ba1640>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845ba1160>\n",
      "Z = NDArray([[[-1.833531]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-1.833531]]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845ba1160>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]])\n",
      "A_t        = tensor([[ 1.3200,  0.8519, -0.9509],\n",
      "        [-0.2586,  0.8289,  0.3701],\n",
      "        [-0.0037,  0.9473,  0.4353],\n",
      "        [-0.3471, -0.6862,  0.2598],\n",
      "        [ 0.7514, -1.2294, -1.8327]])\n",
      "_A         = array([[ 1.3200428 ,  0.8519144 , -0.9508507 ],\n",
      "       [-0.25861153,  0.828859  ,  0.37011066],\n",
      "       [-0.00371555,  ...29144],\n",
      "       [-0.34713647, -0.6862239 ,  0.25979483],\n",
      "       [ 0.75136185, -1.2294345 , -1.8326827 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845c16670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845c16670>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c16130>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c16130>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845c16670>\n",
      "Z = NDArray([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[ 1.3200428   0.8519144  -0.9508507 ]\n",
      " [-0.25861153  0.828859    0.37011066]\n",
      " [-0.00371555  0.9473257   0.43529144]\n",
      " [-0.34713647 -0.6862239   0.25979483]\n",
      " [ 0.75136185 -1.2294345  -1.8326827 ]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845c16670>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.... ]\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]])\n",
      "A_t        = tensor([[[ 0.6772,  0.1087],\n",
      "         [-0.3206, -0.5328],\n",
      "         [ 1.4481,  2.3238]],\n",
      "\n",
      "        [[ 2.6118,  0.0611],\n",
      "...         [-0.4489,  0.2671]],\n",
      "\n",
      "        [[ 0.0928, -0.4465],\n",
      "         [-0.3253,  0.3682],\n",
      "         [ 0.6174,  0.3607]]])\n",
      "_A         = array([[[ 0.6771947 ,  0.10870803],\n",
      "        [-0.32060415, -0.5328463 ],\n",
      "        [ 1.4481194 ,  2.3237994 ]],\n",
      "\n",
      "       [...  [[ 0.09279188, -0.44650966],\n",
      "        [-0.32530743,  0.36815342],\n",
      "        [ 0.6174357 ,  0.36072657]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.... ]\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0...\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845b6f370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0...\n",
      "  [-0.44889545  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845b6f370>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845b6f3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845b6f3d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845b6f370>\n",
      "Z = NDArray([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.061128...45  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[ 0.6771947   0.10870803]\n",
      "  [-0.32060415 -0.5328463 ]\n",
      "  [ 1.4481194   2.3237994 ]]\n",
      "\n",
      " [[ 2.611796    0.061128...45  0.2670805 ]]\n",
      "\n",
      " [[ 0.09279188 -0.44650966]\n",
      "  [-0.32530743  0.36815342]\n",
      "  [ 0.6174357   0.36072657]]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845b6f370>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.... ]\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]])\n",
      "A_t        = tensor([[[-0.4410,  0.0718],\n",
      "         [-1.5023, -0.0198],\n",
      "         [-2.9242,  0.6088]],\n",
      "\n",
      "        [[ 0.6106,  0.8656],\n",
      "...         [-0.9070, -0.3669]],\n",
      "\n",
      "        [[ 0.3390, -0.8299],\n",
      "         [ 0.2380,  0.3032],\n",
      "         [-0.7334, -0.7552]]])\n",
      "_A         = array([[[-0.44101718,  0.07179229],\n",
      "        [-1.5022818 , -0.0197843 ],\n",
      "        [-2.9241614 ,  0.60881114]],\n",
      "\n",
      "       [...  [[ 0.33902806, -0.8299044 ],\n",
      "        [ 0.23796475,  0.3032223 ],\n",
      "        [-0.73337084, -0.7552029 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.... ]\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0...\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc845c7b550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0...\n",
      "  [-0.90701824 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc845c7b550>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c7be80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'amax'\") raised in repr()] Tensor object at 0x7fc845c7be80>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x7fc845c7b550>\n",
      "Z = NDArray([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.865607...24 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "         \u001b[90m# Keepdims makes broadcast work along right axes\u001b[39;49;00m\n",
      ">       Z_max = array_api.amax(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\u001b[0m\n",
      "\n",
      "Z          = NDArray([[[-0.44101718  0.07179229]\n",
      "  [-1.5022818  -0.0197843 ]\n",
      "  [-2.9241614   0.60881114]]\n",
      "\n",
      " [[ 0.6106354   0.865607...24 -0.36688906]]\n",
      "\n",
      " [[ 0.33902806 -0.8299044 ]\n",
      "  [ 0.23796475  0.3032223 ]\n",
      "  [-0.73337084 -0.7552029 ]]], device=cuda())\n",
      "self       = <needle.ops.LogSumExp object at 0x7fc845c7b550>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:410: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-16-16-16]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-8-8-8]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-16-16-32]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-64-64-64]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-72-72-72]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_matmul[cpu-128-128-128]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_broadcast_to[cpu-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_broadcast_to[cpu-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_broadcast_to[cuda-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_broadcast_to[cuda-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_reshape[cpu-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_reshape[cpu-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_reshape[cuda-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_reshape[cuda-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'transpose'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape0-None]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape1-0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape2-1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape3-2]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape0-None]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape1-0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape2-1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape3-2]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'amax'\n",
      "\u001b[31m================ \u001b[31m\u001b[1m70 failed\u001b[0m, \u001b[32m48 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 6.21s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn.py`, implement:\n",
    "- `Flatten`\n",
    "- `Conv`\n",
    "\n",
    "In `python/apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_training.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation for you as a wrapper around your previous `BatchNorm1d` implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = conv(out_grad, W)` \\\n",
    "`W.grad = conv(X, out_grad)`\n",
    "\n",
    "In which the \"\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform`, etc. init functions to support multidimensional arrays.** In particular, you should add a new `shape` argument which is then passed to, e.g., the underlying `rand` function.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. We have added a module called `Flatten` in `nn.py` that you can complete and use, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,\n",
    "         collate_fn=ndl.data.collate_ndarray,\n",
    "         device=device,\n",
    "         dtype=\"float32\")\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "Implement - `Sigmoid`\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn.py`, implement `Sigmoid`, `LSTMCell` and `LSTM`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    " a g m s \n",
    " b h n t \n",
    " c i o u \n",
    " d j p v \n",
    " e k q w \n",
    " f l r x \n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    " a g m s   b h n t \n",
    " b h n t   c i o u \n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_training.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_training import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('codeserver_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aca522a4f3a95a8cc19c0c49aa2b52717208ab4d9caac282bf163cf809ab5536"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
